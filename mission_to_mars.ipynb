{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import datetime as dt\n",
    "import requests as req\n",
    "import datetime as dt\n",
    "from splinter import Browser\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize pymongo client with connection through localhost to M2Mars_db, collection db.news for mars_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection\n",
    "db = client.M2Mars_db\n",
    "collection = db.news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access local nasa news content, open and read html, create news_soup with bs4 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set local path for static html file provided by instr.\n",
    "local_nasa_file = 'news_nasa_mars_exploration_program.html'\n",
    "\n",
    "# open and read local html \n",
    "nasa_html = open(local_nasa_file, \"r\").read()\n",
    "\n",
    "# create bs4 parsed html object\n",
    "news_soup = bs(nasa_html, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate through news_soup to access each attr, save to dictionary and insert to collection ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through news_soup to scrape text date, title and teaser attrs in list_text class, return and\n",
    "# create and insert collection ittems\n",
    "\n",
    "for news in news_soup.find_all('div', attrs={'class': 'list_text'}):\n",
    "    dte = news.find('div', attrs={'class': 'list_date'})\n",
    "    title = news.find('div', attrs={'class': 'content_title'})\n",
    "    teaser = news.find('div', attrs={'class': 'article_teaser_body'})\n",
    "    # create post dict for insert to mongoDB document\n",
    "    post = {\n",
    "        'date': dte.text,\n",
    "        'title': title.text,\n",
    "        'teaser': teaser.text\n",
    "    }\n",
    "    #print(post)\n",
    "    collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat db, collection, open, soup, iterate and insert processes for JPL featured photo ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Go back to assignment to tackle splinter use in JPL photo workflow - N.B. initial bs4 workflow unsuccessful ####\n",
    "\n",
    "# Define database and collection\n",
    "db = client.M2Mars_db\n",
    "collection = db.pic\n",
    "\n",
    "jplPic = []\n",
    "\n",
    "# create chrome browser open with local chromedriver .exe (or set path) and visit target jpl url\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless = False)\n",
    "jpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(jpl_url)\n",
    "\n",
    "# allow 2 sec for page load and click to nav to feature full image via splinter\n",
    "time.sleep(2)\n",
    "browser.click_link_by_partial_text('FULL IMAGE')\n",
    "\n",
    "# allow 5 sec for page load and click to nav to more info\n",
    "time.sleep(5)\n",
    "browser.click_link_by_partial_text('more info')\n",
    "\n",
    "# create next level down soup and access photo url components\n",
    "picHtml = browser.html\n",
    "picSoup = bs(picHtml, 'html.parser')\n",
    "picUrl = picSoup.find('img', class_ = 'main_image')\n",
    "# print(picUrl)\n",
    "picSrc = picUrl.get('src')\n",
    "\n",
    "# create feature photo url with src url from above\n",
    "featured_image_url = \"https://www.jpl.nasa.gov\"+picSrc\n",
    "\n",
    "jplPic = {'featured_image_url': featured_image_url}\n",
    "collection.insert_one(jplPic)\n",
    "\n",
    "browser.quit()\n",
    "# print(featured_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat db, collection, open, soup, iterate and insert processes for NASA Mars twitter for weather report ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reiterate database and redefine collection for NASA Mars twitter weather report\n",
    "db = client.M2Mars_db\n",
    "collection = db.marswx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and request content for mares weather twitter acct\n",
    "marswx = 'https://twitter.com/marswxreport?lang=en'\n",
    "response = req.get(marswx)\n",
    "\n",
    "# Create BeautifulSoup object, parse with 'html.parser'\n",
    "wxsoup = bs(response.text, 'html.parser')\n",
    "\n",
    "#print(wxsoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate through wxsoup to access each attr, save to dictionary and insert to collection ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate li tag, create sub of soup and iterate through tweets to scrape handle, tweet text -- return,\n",
    "# create and insert collection records to mongoDB, dictionary and recent wxrpt\n",
    "\n",
    "wxdict = []\n",
    "wxtweets = wxsoup.find_all('li','js-stream-item')\n",
    "for wx in wxtweets:\n",
    "        wx.find('p','tweet-text')\n",
    "        wx_user = wx.find('span','username').text\n",
    "        wx_text = wx.find('p','tweet-text').text\n",
    "        wxtimestamp = wx.find('a','tweet-timestamp')['title']\n",
    "        wxdict.append({'handle': wx_user, 'wxrpt': wx_text, 'wxtime': wxtimestamp})\n",
    "        post = {\n",
    "            'handle': wx_user,\n",
    "            'wxrpt': wx_text,\n",
    "            'wxtime': wxtimestamp\n",
    "        }\n",
    "        #collection.insert_one(post)\n",
    "        # verify tweet has a wx and break if true, insert post\n",
    "        if \"Sol \" in wx_text:\n",
    "            mars_weather = wx_text\n",
    "            collection.insert_one(post)\n",
    "            break\n",
    "            \n",
    "#mars_weather       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect space-facts.com mars tabular data to pandasDF and output html table string ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read space-facts/mars html into DF\n",
    "url = ('https://space-facts.com/mars/')\n",
    "match = 'Equatorial Diameter:'\n",
    "facts = pd.read_html(url, match = match)\n",
    "factsDF = facts[0]\n",
    "\n",
    "# factsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and output factsDF data to html table\n",
    "with open('mars_facts.html', 'w') as mf:\n",
    "    factsDF.to_html(mf)\n",
    "    \n",
    "# print(factsDF.to_html()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Mars Hemishere image urls and titles from USGS page, create dictionary of img_url and title ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reiterate database and redefine collection USGS hemisphere photo urls\n",
    "db = client.M2Mars_db\n",
    "collection = db.hemis\n",
    "\n",
    "# identify USGS mars hemishpere resource\n",
    "usgs_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# Retrieve page with requests\n",
    "response = req.get(usgs_url)\n",
    "\n",
    "# Create BeautifulSoup object, parse with 'html.parser'\n",
    "usgsSoup = bs(response.text, 'html.parser')\n",
    "\n",
    "# Set list for data collection and dictionary creation\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Soup for 4 hemispheres, i.e. landing page target class\n",
    "fourHSoup = usgsSoup.find_all('a', class_='itemLink product-item') \n",
    "\n",
    "#iterate through soup to capture image title, build url to hemishphere page, retrieve image src and load dictionary\n",
    "for item in fourHSoup:\n",
    "    urlBack = item['href']\n",
    "    title = item.find('div', attrs={'class': 'description'}).text\n",
    "    urlTo = \"https://astrogeology.usgs.gov\"+urlBack\n",
    "    #print(title, urlTo)\n",
    "    # follow urlTo down a level with get -- create soup for 1 hemisphere per iter,  write dict and insert to collection\n",
    "    response = req.get(urlTo)\n",
    "    oneHSoup = bs(response.text, 'html.parser')\n",
    "    findSrc = oneHSoup.find('div', class_ ='downloads')\n",
    "    oneUrl = findSrc.find('a')['href']\n",
    "    post = {\n",
    "        'title': title,\n",
    "        'img_url': oneUrl\n",
    "    }\n",
    "    hemisphere_image_urls.append({'title': title, 'img_url': oneUrl})\n",
    "    collection.insert_one(post)\n",
    "#hemisphere_image_urls  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
